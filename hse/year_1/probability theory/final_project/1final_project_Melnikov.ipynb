{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CnUdDZ4onHd"
      },
      "source": [
        "# Probability\n",
        "## Final project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVsd_-pwonHh"
      },
      "source": [
        "In the final project you will develop your own classification algorithm based on probabilistic models.\n",
        "\n",
        "In `data.zip`you can find a collection of texts in English, Italian, Spanish, German, French, Polish and Portuguese languages obtained from random Wikipedia articles, see e.g. `Spanish.txt`. (See corresponding `.source.txt` files and links therein for lists of authors.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QGomL4UnonHj",
        "outputId": "03e9be40-7ea3-4090-8977-4c5d26c22d5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tomas zapata sierra medellin colombia  de mayo de  es productor de cine y de teatrocita requerida\n",
            "sad eyed lady of the lowlands en espanol senorita de ojos tristes de las tierras bajas es una cancion compuesta por el cantante estadounidense bob dylan fue incluida en el album blonde on blonde editado el  de mayo de \n",
            "la revista mojo la coloco en el puesto  de su lista de las  mejores canciones de bob dylan\n",
            "calyptocephalella canqueli es una especie extinta de anfibio anuro perteneciente al genero c\n"
          ]
        }
      ],
      "source": [
        "with open(\"Spanish.txt\") as f:\n",
        "    print(f.read(500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "in7YYpE4onHk",
        "outputId": "75ecf5ff-90cc-4fdc-cb54-16e6dd01fbb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "riedhofe ist der name von ortsteilen in deutschland\n",
            "\n",
            "in badenwurttemberg\n",
            "riedhofe langenau ortsteil der stadt langenau im albdonaukreis\n",
            "riedhofe frickingen ortsteil der gemeinde frickingen im bodenseekreis\n",
            "riedhofe riegel am kaiserstuhl ortsteil der gemeinde riegel am kaiserstuhl im landkreis emmendingen\n",
            "riedhofe kongen ortsteil der gemeinde kongen im landkreis esslingen\n",
            "riedhofe leingarten ortsteil der gemeinde leingarten im landkreis heilbronn\n",
            "riedhofe bad wurzach ortsteil der stadt bad wurzac\n"
          ]
        }
      ],
      "source": [
        "with open(\"German.txt\") as f:\n",
        "    print(f.read(500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSlexPmBonHl"
      },
      "source": [
        "This is our training data. These texts are preprocessed: only standard Latin characters kept, diacritics removed, punctuations removed, all letters converted to lowercase. We will use similar preprocessing for new texts that we have to classify. Here are some useful functions to do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YtkQsA0onHm"
      },
      "source": [
        "### Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9pmmgKsZonHn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "### FROM: https://stackoverflow.com/a/518232/3025981\n",
        "def strip_accents(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "### END FROM\n",
        "\n",
        "def clean_text(s):\n",
        "    return re.sub(\"[^a-z \\n]\", \"\", strip_accents(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or23-0YeonHo"
      },
      "source": [
        "### Learning: obtaining character frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PncO_AQonHp"
      },
      "source": [
        "First of all, we have to find character relative frequencies in texts of each language. We will consider them as probability for character to appear in our multinomial model. Write function `get_freqs(text, relative)` that takes string `text` as input and returns dictionary which keys are all distinct characters occurred in `text` and values are frequencies (relative if `relative` is `True` and absolute otherwise). A similar function was discussed in Python videos previously. Note that Python treat `str` objects as a sequence of characters (e.g. if you try to iterate it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "JBA_XsgYonHq",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bcf26fb5354210c1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_freqs(text, relative=False):\n",
        "    ### BEGIN SOLUTION\n",
        "    n = len(text)\n",
        "    frequency = {}\n",
        "    for letter in text:\n",
        "        frequency[letter] = frequency.setdefault(letter, 0) + 1\n",
        "    if relative:\n",
        "        for key in frequency:\n",
        "            frequency[key] /= n\n",
        "    return frequency\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "swbANAoSonHq",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-122fc498050c5088",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert get_freqs('Hello, World!') == {'H': 1, 'e': 1, 'l': 3, 'o': 2, ',': 1,\n",
        "                                      ' ': 1, 'W': 1, 'r': 1, 'd': 1, '!': 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60jvwbNuonHr"
      },
      "source": [
        "Now use function `get_freqs` to create a dictionary `lang_to_prob` which keys are names of languages (i.e. `'English'`, `'Italian'`, `'Spanish'`, `'German'`, `'French'`, `'Polish'`, `'Portuguese'`) and values are dictionaries of relative frequencies, obtained by processing of corresponding `.txt` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "z9DuhKWQonHr",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1beba6e8ac03b0b0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "lang_to_probs = {}\n",
        "languages = ['English', 'Italian', 'Spanish', 'German', 'French', 'Polish', 'Portuguese']\n",
        "for lang in languages:\n",
        "    lang_to_probs.setdefault(lang, {})\n",
        "    text = ''\n",
        "    with open(f\"{lang}.txt\") as f:\n",
        "        for line in f:\n",
        "            text += line\n",
        "    lang_to_probs[lang] = get_freqs(text, relative=True)\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "K_6dUv1JonHs",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-56feb4b2ab62c15c",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert abs(lang_to_probs['Polish']['a'] - 0.08504245058355897) < 0.00001\n",
        "assert abs(lang_to_probs['English']['x'] - 0.001387857977519179) < 1e-5\n",
        "assert len(set(lang_to_probs['Portuguese'])) == 28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IT9Az6EonHs"
      },
      "source": [
        "### Likelihoods\n",
        "Now let us start implementing the actual classifier. We begin with multinomial likelihood function. Implement function `multinomial_likelihood(probs, freqs)` that takes two arguments: `probs` are dictionary of probabilities of each character (in some language) and `freqs` is dictionary of absolute frequencies of each character (in some text we want to classify). This function have to return probability to obtain these absolute frequencies from multinomial distribution with given probabilities $P((X_1 = f_1) \\cap (X_2 = f_2) \\cap \\ldots \\cap (X_k = f_k))$ provided that $(X_1, \\ldots, X_k)$ is a system of multinomially distributed values with probabilities $(p_1, \\ldots, p_k)$. (You need `factorial` function that can be imported from `math`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "KS4lto2KonHs",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-9a0431bd007d04cf",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "from math import factorial, log, e\n",
        "def multinomial_likelihood(probs, freqs):\n",
        "    n = 0\n",
        "    numerator = 1\n",
        "    denominator = 1\n",
        "    for char, r in freqs.items():\n",
        "        numerator *= probs[char]**r\n",
        "        denominator *= factorial(r)\n",
        "        n += r\n",
        "    probability = factorial(n)\n",
        "    probability *= numerator\n",
        "    probability /= denominator\n",
        "    return probability\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q3K8je2onHt"
      },
      "source": [
        "Let's find likelihood of data with frequencies `{'a': 2, 'b': 1, 'c': 2}` and probabilities `{'a': 0.2, 'b': 0.5, 'c': 0.3}`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "QoqTTC0ionHt",
        "outputId": "77f28514-5778-4ede-c587-6a4b33ae65c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05400000000000001"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "multinomial_likelihood(probs={'a': 0.2, 'b': 0.5, 'c': 0.3}, freqs={'a': 2, 'b': 1, 'c': 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "2VYIB_LronHt",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-0a8a6d99346b6bf8",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert abs(multinomial_likelihood(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                    freqs={'a': 2, 'b': 1, 'c': 2}) - 0.054) < 0.000001\n",
        "assert abs(multinomial_likelihood(probs={'a': 0.2, 'b': 0.1, 'c': 0.3, 'd': 0.4},\n",
        "                    freqs={'a': 2, 'b': 1, 'c': 2}) - 0.0108) < 0.000001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8csAFr_YonHu"
      },
      "source": [
        "Note that the coefficient with factorials depends only on `freqs` (i.e. text that we analyse) and does not depend on `probs`. It means that for all possible languages this coefficient will be the same. As we are going to consider fixed text and compare likelihoods for different languages, we see, that we do not need this coefficient in most of cases. Let us implement function `multinomial_likelihood_without_coeff` that returns the same probability as `multinomial_likelihood` but without coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "CzH6sstIonHu",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-99bfc1e01b6b804e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def multinomial_likelihood_without_coeff(probs, freqs):\n",
        "    probability = 1\n",
        "    for char, r in freqs.items():\n",
        "        probability *= probs[char]**r\n",
        "\n",
        "    return probability\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH5vtbPhonHu"
      },
      "source": [
        "Corresponding probability become smaller:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "AWxoGtbKonHu",
        "outputId": "3396e583-ae3b-4f08-8e5a-97bdaf4027ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0018000000000000004"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                                     freqs={'a': 2, 'b': 1, 'c': 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "RVn4twczonHv",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-3de0433027c5af68",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert multinomial_likelihood_without_coeff(\n",
        "    probs={'a': 0.3, 'b': 0.4, 'c': 0.3},\n",
        "    freqs={'a': 2, 'b': 1, 'c': 2}) == 0.00324\n",
        "assert abs(multinomial_likelihood_without_coeff(\n",
        "    probs={'a': 0.3, 'b': 0.4, 'c': 0.3},\n",
        "    freqs={'a': 2, 'b': 1, 'c': 5}) - 8.747999999999e-05) < 1e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j68-jIAonHv"
      },
      "source": [
        "Actually, likelihoods become extremely small very quickly when we increase absolute frequencies in data. It is not surprisingly: the probability to get the text that coincides with our actual text from random experiment we discussed is extremely small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "QC4HF-UaonHv",
        "outputId": "2033dcbf-4558-4482-e689-0ee120de30c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00018000000000000004"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                    freqs={'a': 3, 'b': 2, 'c': 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "8nSWUd2fonHv",
        "outputId": "79696a88-f0bf-4f6a-9e70-fae20156e726",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.866455078125001e-10"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                    freqs={'a': 3, 'b': 20, 'c': 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i57kmLponHv"
      },
      "source": [
        "Due to limited precision of computer arithmetic, for frequencies large enough we will get exactly zero likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "cWeiFPtvonHv",
        "outputId": "0f8c80fd-5a07-4b71-ecf9-d83f199cc4f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                                     freqs={'a': 543, 'b': 512, 'c': 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzglPanqonHw"
      },
      "source": [
        "Thus we usually cannot use likelihoods like this directly. Common way to deal with such a tiny numbers is to use _logarithms_ instead of likelihood themself. Indeed, logarithm is monotonically increasing function. If we compare logarithms, it is equivalent to compare their arguments.\n",
        "\n",
        "### Log likelihood\n",
        "Implement function `log_likelihood_without_coeff(probs, freqs)` that calculates logarithm of likelihood (without factorial coefficient). Note that you cannot simply put `multinomial_likelihood_without_coeff` into `log`: if the likelihood would be extremely small (equal to zero from computer's point of view), `log` of it will be negative infinity. Thus we have to algebraically transform the expression for log likelihood first. Let us use properties of logarithm to do it: $\\log (ab) = \\log a + \\log b$, $\\log (a^b)=b\\log a$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "DjeFHu8MonHw",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c6283b47bbcf5b95",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "def log_likelihood_without_coeff(probs, freqs):\n",
        "    log_likelihood = 0\n",
        "    for char, r in freqs.items():\n",
        "        log_likelihood += r*log(probs[char])\n",
        "\n",
        "    return log_likelihood\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQyt3Yu9onHw"
      },
      "source": [
        "Likelihoods are probabilities, so they are less than 1 and their logarithms are negative. The larger absolute value of log-likelihood, the less is likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5bS5HxvponHw",
        "outputId": "959c250b-69bf-44e7-8845-1e32b53783f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-6.319968614080018"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                             freqs={'a': 2, 'b': 1, 'c': 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1E5IspSonHw"
      },
      "source": [
        "Now we can deal with inputs that lead to exact zero value previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "iToxFlQxonHw",
        "outputId": "5f0f0ad9-5c13-418e-f026-c508ff5d44b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1231.2240885070605"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                             freqs={'a': 543, 'b': 512, 'c': 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "6yjncsWSonHx",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-38d4478fc7c1656b",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                              freqs={'a': 2, 'b': 1, 'c': 2}) + 6.319968614080018) < 0.00001\n",
        "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                             freqs={'a': 543, 'b': 512, 'c': 2}) + 1231.2240885070605) < 0.0001\n",
        "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                             freqs={'a': 543, 'b': 512}) + 1228.8161428984085) < 0.0001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36qOaUfbonHx"
      },
      "source": [
        "### Theoretical question: cross-entropy and Jensen's inequality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSyyRqBnonHx"
      },
      "source": [
        "### Maximum likelihood classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq0kYJJUonHx"
      },
      "source": [
        "The function that you obtained is also know as *cross entropy* and is extremely popular in machine learning, namely, in classification problems. It allows you to measure how good probability distribution $(p_1, \\ldots, p_k)$ fits to actual absolute frequencies obtained from the data $(f_1, \\ldots, f_k)$.\n",
        "\n",
        "Assume that frequencies $(f_1, \\ldots, f_k)$ are fixed. What is the best distribution $(p_1, \\ldots, p_k)$ from likelihood's perspective?\n",
        "\n",
        "Intuitively, it seems that we have to put relative frequencies\n",
        "$$r_i = \\frac{f_i}{\\sum_{j=1}^k f_j}$$\n",
        "as $p_i$ to get best fit. In fact, it is true. To prove it, let us use Jensen's inequality for logarithms. It is stated as follows:\n",
        "\n",
        "For any values $\\alpha_1, \\ldots, \\alpha_k$, such that $\\sum_{j=1}^k \\alpha_j = 1$ and $\\alpha_j \\ge 0$ for all $j=1, \\ldots, k$, and any positive values $x_1, \\ldots, x_k$, the following inequality holds:\n",
        "\n",
        "$$\\log \\sum_{j=1}^k \\alpha_j x_j \\ge \\sum_{j=1}^k \\alpha_j\\log(x_j).$$\n",
        "\n",
        "Use this inequality to prove that\n",
        "$$\\sum_{j=1}^k r_j \\log p_j - \\sum_{j=1}^k r_j \\log r_j \\le 0,$$\n",
        "then prove that to obtain maximum log-likelihood (and therefore maximum likelihood) for fixed $(f_1, \\ldots, f_k)$ we have to put $p_i=r_i$, $i=1,\\ldots, k$.\n",
        "\n",
        "**Hint:** use properties of logarithm to transform left-hand part of the last inequality to right-rand part of the previous inequality.\n",
        "\n",
        "(Submit your proof as staff graded assignment.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Proof**\n",
        "$\\sum_{j=1}^k r_j \\log p_j - \\sum_{j=1}^k r_j \\log r_j =\n",
        "\\sum_{j=1}^k (r_j \\log p_j - r_j \\log r_j) =\\\\\n",
        "\\sum_{j=1}^k r_j( \\log p_j - \\log r_j) =\n",
        "\\sum_{j=1}^k r_j \\log \\frac{p_j}{r_j} $\n",
        "\n",
        "According to Jensen's inequality we can say that\\\n",
        "$\\sum_{j=1}^k r_j \\log \\frac{p_j}{r_j} \\le \\log\\sum_{j=1}^k r_j \\frac{p_j}{r_j}\\\\\n",
        "\\sum_{j=1}^k r_j \\log \\frac{p_j}{r_j} \\le \\log\\sum_{j=1}^k p_j$\\\n",
        "As $\\sum_{j=1}^k p_j = 1$ we get\\\n",
        "$\\sum_{j=1}^k r_j \\log \\frac{p_j}{r_j} \\le \\log1\\\\\n",
        "\\sum_{j=1}^k r_j \\log \\frac{p_j}{r_j} \\le 0$\n",
        "\n",
        "From this we can conclude that\n",
        "$$\\sum_{j=1}^k r_j \\log p_j - \\sum_{j=1}^k r_j \\log r_j\\le 0$$\n",
        "and that\n",
        "$$\\sum_{j=1}^k r_j \\log p_j \\le \\sum_{j=1}^k r_j \\log r_j.$$\n",
        "\n",
        "\n",
        "#Log-likelihood function\n",
        "$L((f_1,..,f_k),(p_1,..,p_k)) = \\sum_{j=1}^k f_j\\cdot \\log p_j$.\\\n",
        "Let's introduce relative frequencies $r_j$ into the formula for $L$.\\\n",
        "$r_j = \\dfrac{f_j}{\\sum_{i=1}^k f_i} ⇒f_j = r_j \\sum_{i=1}^k f_i$\\\n",
        "Sum of frequencies is a constant for any given text, so let's denote it as constant $F$ :\\\n",
        "$F = \\sum_{i=1}^k f_i$.\\\n",
        "Then we get:\\\n",
        "$f_j = F \\cdot r_j$.\\\n",
        "Let's rewrite $L$ with respect to $r_j$:\n",
        "$$L((f_1,..,f_k),(p_1,..,p_k)) = \\sum_{j=1}^k F\\cdot r_j \\log p_j,$$\n",
        "and factor $F$ out of the sum:\n",
        "$$L((f_1,..,f_k),(p_1,..,p_k)) = F \\cdot \\sum_{j=1}^k r_j \\log p_j$$\n",
        "As we've already proved\n",
        "$$\\sum_{j=1}^k r_j \\log p_j \\le \\sum_{j=1}^k r_j \\log r_j.$$\n",
        "So if we multiply both sides by $F$ we get\n",
        "$$F \\sum_{j=1}^k r_j \\log p_j \\le F \\sum_{j=1}^k r_j \\log r_j,$$\n",
        "which basically means that\n",
        "$$L((f_1,..,f_k),(p_1,..,p_k)) \\le  L((f_1,..,f_k),(r_1,..,r_k)).$$\n",
        "\\\n",
        "We can say that values of $L((f_1,..,f_k),(p_1,..,p_k))$ are capped by $L((f_1,..,f_k),(r_1,..,r_k))$\\\n",
        "and it's maximum is reached when\n",
        "$(p_1,..,p_k) = (r_1,..,r_k)$."
      ],
      "metadata": {
        "id": "FaFAGMZAYOZ-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl0iOhJPonHx"
      },
      "source": [
        "Now we will use likelihood to choose the best language for some text we want to classify. Write function `mle_best(text, lang_to_probs)` that takes some `text` and dictionary `lang_to_probs` that we created previously and returns the name of language such that likelihood of our data for this language is maximal. Note that you have to preprocess  `text` using function `clean_text` before finding frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "dM8wxxcRonHx",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-761e62cc0f17dc2d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "def mle_best(text, lang_to_probs):\n",
        "    text = clean_text(text)\n",
        "    likelihood = {}\n",
        "    for lang in languages:\n",
        "        likelihood[lang] = log_likelihood_without_coeff(lang_to_probs[lang], get_freqs(text))\n",
        "    #print(likelihood)\n",
        "    best = max(likelihood, key=lambda x: likelihood[x])\n",
        "    return best\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5BY3K9onHx"
      },
      "source": [
        "Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "zcIgYjYEonHx",
        "outputId": "a857c5b7-b071-4dda-b683-081489be82a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'English'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# Source: https://en.wikipedia.org/wiki/1134_Kepler\n",
        "text = \"\"\"1134 Kepler, provisional designation 1929 SA, is a stony asteroid\n",
        "and eccentric Mars-crosser from the asteroid belt, approximately\n",
        "4 kilometers in diameter\"\"\"\n",
        "mle_best(text, lang_to_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "eebo3vi7onHy",
        "outputId": "840ff460-4d0f-442b-da67-e38ca1983d02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Polish'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# Source: https://pl.wikipedia.org/wiki/(1134)_Kepler\n",
        "text = \"\"\"\"(1134) Kepler – planetoida z grupy przecinających\n",
        "orbitę Marsa okrążająca Słońce w ciągu 4 lat i 145 dni\n",
        "w średniej odległości 2,68 au.\n",
        "\"\"\"\n",
        "mle_best(text, lang_to_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "z5-vM4T5onHy",
        "outputId": "84173da8-330b-461e-92b6-cc4fb47f46ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Italian'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "# Source: https://it.wikipedia.org/wiki/1134_Kepler\n",
        "text = \"\"\"1134 Kepler è un asteroide areosecante. Scoperto nel 1929,\n",
        "presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098\n",
        "UA e da un'eccentricità di 0,4651458, inclinata di 15,17381° rispetto\n",
        "\"\"\"\n",
        "mle_best(text, lang_to_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "2_0M8_p8onHy",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-b83cf8d4bcee2ab1",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "lines = ['Aaa', 'Aa', 'Kepler è un asteroide areosecante. Scoperto nel',\n",
        "        \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \"\n",
        "         \"UA e da un'eccentricità di 0,4651458, inclinata\",\n",
        "         \"Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i \",\n",
        "         \"Kepler, provisional designation 1929 SA, is a stony asteroid \"\n",
        "         \"and eccentric Mars-crosser from the\"]\n",
        "assert mle_best(lines[0], lang_to_probs) == 'Portuguese'\n",
        "assert mle_best(lines[1], lang_to_probs) == 'Portuguese'\n",
        "assert mle_best(lines[2], lang_to_probs) == 'French'\n",
        "assert mle_best(lines[3], lang_to_probs) == 'Italian'\n",
        "assert mle_best(lines[4], lang_to_probs) == 'Polish'\n",
        "assert mle_best(lines[5], lang_to_probs) == 'English'\n",
        "assert mle_best(\"Aaaa\", lang_to_probs) == \"Portuguese\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7dyK3uJonHy"
      },
      "source": [
        "### Look, it works!\n",
        "Fantastic! Just a couple of equations, and we created fully automatic language detection algorithm!\n",
        "\n",
        "Now let us make it even better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYWQTULZonHy"
      },
      "source": [
        "### Let's go Bayes\n",
        "Assume that we selected random article from all Wikipedia articles in languages that we consider. There are different number of articles in different languages, so it is more likely to obtain article e.g. in English than in Polish (at least, at this moment). It means that we need stronger evidence for Polish to accept it comparing with English. To take into account this information, we will use Bayes' rule as discussed in the videos.\n",
        "\n",
        "Recall that in Bayessian approach we consider _prior_ probabilities of languages, then use Bayes' rule to find their posterior probabilies and select language with largest posterior probability. We begin with finding prior probabilities. Create a dictionary `lang_to_prior` which keys are language names and values are prior probabilities. Assume that priors are prortional to number of articles in each language (in thousands). Let us use these numbers (in thousands): English: 6090, Italian: 1611, Spanish: 1602, German: 2439, French: 2222, Polish: 1412, Portuguese: 1034. Remember that all prior probabilities should sum up to 1!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "RnDs7oOeonHy",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1076e90562dad99c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "lang_to_prior = {'English':6090, 'Italian':1611, 'Spanish':1602, 'German':2439, 'French':2222, 'Polish':1412, 'Portuguese':1034}\n",
        "n = sum(lang_to_prior.values())\n",
        "for lang in lang_to_prior:\n",
        "    lang_to_prior[lang] /= n\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "o4QVmRdzonHz",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-19765ecb964a7f7f",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert abs(lang_to_prior['French'] - 0.13540524070688603) < 0.000001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXhgYKFJonHz"
      },
      "source": [
        "Now implement function `bayesian_best(text, lang_to_probs, lang_to_prior)` that takes some text `text`, dictionary `lang_to_probs` created before and `lang_to_prior` with prior probabilities. This function have to return language name with largest posterior probability. Note that as we only compare posterior probabilities, we can ignore the denominator in Bayes' rule: it is the same for all languages. Note also that we need to work with logarithms of posterior instead of posteriors themself to avoid extremely small numbers. Use properties of logarithm to do it effeciently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "A7AFgrY9onHz",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1725db02a8cb77ad",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "def bayesian_best(text, lang_to_probs, lang_to_prior):\n",
        "    text = clean_text(text)\n",
        "    log_likelihood = {}\n",
        "    posterior_log = {}\n",
        "    for lang in languages:\n",
        "        log_likelihood[lang] = log_likelihood_without_coeff(lang_to_probs[lang], get_freqs(text))\n",
        "        posterior_log[lang] = log_likelihood[lang]+log(lang_to_prior[lang])\n",
        "\n",
        "    best = max(posterior_log, key=lambda x: posterior_log[x])\n",
        "    return best\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro9QObUJonHz"
      },
      "source": [
        "For example, MLE algorithm believes that word `\"The\"` belongs go German language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "N6_gz6rbonHz",
        "outputId": "adb12042-9103-457b-e037-32faaa05cf7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'German'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "mle_best(\"The\", lang_to_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCH74x-GonHz"
      },
      "source": [
        "However, if we take into account that English is more popular in Wikipdia, results changes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "v8LfC6ToonHz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c6afa94d-a618-4f9e-d2fb-e5b7857ceb04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'English'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "bayesian_best(\"The\", lang_to_probs, lang_to_prior)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "KvlT6sBConHz",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-d6b6b54026f24fb0",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "lines = ['Aaa', 'Aa', 'Kepler è un asteroide areosecante. Scoperto nel',\n",
        "        \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \"\n",
        "         \"UA e da un'eccentricità di 0,4651458, inclinata\",\n",
        "         \"Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i \",\n",
        "         \"Kepler, provisional designation 1929 SA, is a stony asteroid \"\n",
        "         \"and eccentric Mars-crosser from the\"]\n",
        "answers = ['English', 'English', 'French', 'Italian', 'Polish', 'English']\n",
        "for line, answer in zip(lines, answers):\n",
        "    assert bayesian_best(line, lang_to_probs, lang_to_prior) == answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9RMCkxfonH0"
      },
      "source": [
        "### Measuring uncertainty\n",
        "Finally, let us get posterior probability how certain our Bayesian algorithm in its classification. Implement function `bayesian_posterior(text, lang_to_probs, lang_to_prior, test_lang)` that takes some `text`, `lang_to_probs`, `lang_to_prior` and a particular language `test_lang` we are interested in and returns posterior probability for this language provided this text. Use formula for the posterior from the videos.\n",
        "\n",
        "Note that in this case you have to work with actual probabilites (likelihoods), not their logarithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "c0epuiy4onH0",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-8705f891f8eff4f7",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTIONS\n",
        "def bayesian_posterior(text, lang_to_probs, lang_to_prior, test_lang):\n",
        "    text = clean_text(text)\n",
        "    log_likelihood = {}\n",
        "    posterior_log = {}\n",
        "    denominator = 0\n",
        "    for lang in languages:\n",
        "        log_likelihood[lang] = log_likelihood_without_coeff(lang_to_probs[lang], get_freqs(text))\n",
        "        posterior_log[lang] = log_likelihood[lang]+log(lang_to_prior[lang])\n",
        "        denominator += e**posterior_log[lang]\n",
        "    denominator = log(denominator)\n",
        "    posterior = posterior_log[test_lang] - denominator\n",
        "    posterior = e**posterior\n",
        "    return posterior\n",
        "\n",
        "### END SOLUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "mAOwAYnponH0",
        "outputId": "c3183681-7913-4287-db2e-f047dca6b103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26863814640453415"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"German\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "sUVZPTiPonH0",
        "outputId": "b97c68f1-cedb-4caf-f555-1f62515dcead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5346266046898717"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"English\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "NdIeqtygonH0",
        "outputId": "069b0399-2b45-455b-87dc-e01e4ce60fd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36596793151737494"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"English\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "pm21A6zvonH0",
        "outputId": "5670d17d-eaaa-4489-9172-c02097d87f2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12095519926831601"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"German\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "fsuKSxNmonH0",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-23ecc58928d8d618",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert abs(bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"German\") - 0.26863814640) < 0.00001\n",
        "assert abs(bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"English\") - 0.534626604) < 0.00001\n",
        "assert abs(bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"English\") - 0.365967931517) < 0.00001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NY-Bm5lonH0"
      },
      "source": [
        "We see that our algorithm believes that `\"Das\"` belongs to English. This is due to small amount of data (only three letters!) and high prior for English. However, it is not very certain: the posterior is only 0.37. On the other hand, `\"The\"` belongs to `\"English\"` with much larger posterior probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kiewa34NonH0"
      },
      "source": [
        "## Conclusions\n",
        "Today we used our knowledge of probability to construct our own classifier algorithm. Actually, it is a version of well-known naive Bayesian classifier. Of course, this classifier is far from perfect: for example, it completely ignores words and deal only with characters and their frequencies. Nevertheless, it works rather good despite being so simple. Also it shows several important concepts: likelihood, maximum likelihood estimation, Bayesian estimations and so on.\n",
        "\n",
        "Now you are ready for more sophisticated topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovy3GARjonH0"
      },
      "source": [
        "### P.S. Efficiency considerations\n",
        "During this project we used dictionaries to represent frequency tables and pure Python constructions like loops to process them. However, it is much more effecient to use `numpy` arrays and vectorized functions. You can try to redo this project with `numpy` if you know how to use it. If you don't know yet, that's not a problem: you will learn it soon."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}